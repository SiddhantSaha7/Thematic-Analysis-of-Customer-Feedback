{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d669a70-da64-4caf-bebd-861a3e6b160b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arth/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "# Machine Learning & Clustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# NLP & GenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "\n",
    "# # Configuration\n",
    "# RANDOM_STATE = 42\n",
    "# np.random.seed(RANDOM_STATE)\n",
    "# pd.set_option('display.max_colwidth', 150)\n",
    "\n",
    "# # Select Device (GPU if available, else CPU)\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef210e1-76e0-47ac-9c20-f17590817208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# STRICTLY set this to 1 to prevent kernel crashes on clustering\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03815b1-7691-4fb5-af90-1896ab00f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = '../dataset/Tweets.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"Dataset Successfully Loaded: {df.shape[0]} rows.\")\n",
    "\n",
    "# Cleaning & Preprocessing \n",
    "def clean_tweet(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    # 1. Change to lowercase\n",
    "    text = text.lower()\n",
    "    # 2. Remove mentions (for e.g, @United)\n",
    "    text = \" \".join([word for word in text.split() if not word.startswith('@')])\n",
    "    # 3. Remove whitespace\n",
    "    return text.strip()\n",
    "\n",
    "# Apply cleaning\n",
    "df['cleaned_text'] = df['text'].apply(clean_tweet)\n",
    "\n",
    "# Filter out empty tweets after cleaning\n",
    "df = df[df['cleaned_text'].str.len() > 5].reset_index(drop=True)\n",
    "\n",
    "df_sample = df.copy()\n",
    "\n",
    "print(f\"Data Cleaned & Ready. Working with {len(df_sample)} tweets.\")\n",
    "display(df_sample[['airline_sentiment', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61934ac-da4f-4e5f-bbce-1f5989992c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading SBERT model...\")\n",
    "# 'all-MiniLM-L6-v2' is optimized for speed and performance\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "print(\"Generating embeddings for all tweets...\")\n",
    "embeddings = embedder.encode(df_sample['cleaned_text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "print(f\"Embedding Shape: {embeddings.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca0dca-206a-4b2b-b7e0-bd9f6b8c6d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions to preserve 95% variance\n",
    "pca = PCA(n_components=0.95, random_state=RANDOM_STATE)\n",
    "embeddings_pca = pca.fit_transform(embeddings)\n",
    "\n",
    "print(f\"Original Dimensions: {embeddings.shape[1]}\")\n",
    "print(f\"Reduced Dimensions: {embeddings_pca.shape[1]}\")\n",
    "print(f\"Explained Variance Ratio: {np.sum(pca.explained_variance_ratio_):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead85713-da83-425d-8158-ccc64cf7f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining best K value for K Means\n",
    "results = []\n",
    "for k in range(2, 31):\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(embeddings_pca)\n",
    "\n",
    "    sil = silhouette_score(embeddings_pca, labels)\n",
    "    db = davies_bouldin_score(embeddings_pca, labels)\n",
    "    inertia = km.inertia_\n",
    "\n",
    "    results.append((k, sil, db, inertia))\n",
    "\n",
    "    print(f\"k={k}: silhouette={sil:.4f}, db={db:.4f}, inertia={inertia:.2f}\")\n",
    "\n",
    "ks = [r[0] for r in results]\n",
    "sil_scores = [r[1] for r in results]\n",
    "db_scores = [r[2] for r in results]\n",
    "inertias = [r[3] for r in results]\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(ks, sil_scores)\n",
    "plt.title(\"Silhouette Score\")\n",
    "plt.xlabel(\"k\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(ks, db_scores)\n",
    "plt.title(\"Davies-Bouldin Score\")\n",
    "plt.xlabel(\"k\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(ks, inertias)\n",
    "plt.title(\"Elbow (Inertia)\")\n",
    "plt.xlabel(\"k\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Max silhouette\n",
    "best_sil_k = ks[np.argmax(sil_scores)]\n",
    "\n",
    "# Min DB index\n",
    "best_db_k = ks[np.argmin(db_scores)]\n",
    "\n",
    "print(f\"Best k by Silhouette Score: {best_sil_k}\")\n",
    "print(f\"Best k by Daviesâ€“Bouldin Score: {best_db_k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4773a16d-6ff3-4314-a084-ff20b44dfbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This value was determined after the analysis above\n",
    "num_clusters = 17\n",
    "\n",
    "print(f\"Running KMeans with {num_clusters} clusters...\")\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=RANDOM_STATE, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(embeddings_pca)\n",
    "\n",
    "# Add labels to DataFrame\n",
    "df_sample['kmeans_cluster'] = kmeans_labels\n",
    "\n",
    "# Evaluation Metrics\n",
    "sil_score = silhouette_score(embeddings_pca, kmeans_labels)\n",
    "db_score = davies_bouldin_score(embeddings_pca, kmeans_labels)\n",
    "\n",
    "print(\"KMeans Results\")\n",
    "# The higher the silhoutte score, the better, while opposite is true for Davies-Boudin score.\n",
    "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
    "print(f\"Davies-Bouldin Score: {db_score:.4f}\")\n",
    "\n",
    "# Show distribution of clusters\n",
    "print(\"\\nCluster Distribution:\")\n",
    "print(df_sample['kmeans_cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528365ce-4e4e-4a0f-b827-a03aae4ff7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up the figure size\n",
    "plt.figure(figsize=(18, 7))\n",
    "\n",
    "# --- Plot 1: Cluster Distribution (Bar Chart) ---\n",
    "plt.subplot(1, 2, 1)\n",
    "# Calculate counts for sorted display\n",
    "counts = df_sample['kmeans_cluster'].value_counts().sort_index()\n",
    "sns.barplot(x=counts.index, y=counts.values, palette='viridis')\n",
    "plt.title(f'Distribution of {num_clusters} Clusters')\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# --- Plot 2: Clusters on PCA Components (Scatter Plot) ---\n",
    "# We take the first two dimensions of the PCA embeddings for visualization\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(\n",
    "    x=embeddings_pca[:, 0], \n",
    "    y=embeddings_pca[:, 1], \n",
    "    hue=kmeans_labels, \n",
    "    palette='tab20',  # 'tab20' has enough distinct colors for 17 clusters\n",
    "    s=50, \n",
    "    alpha=0.6,\n",
    "    legend='full'\n",
    ")\n",
    "plt.title('Clusters Visualized on First 2 PCA Components')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Cluster ID', ncol=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd998f9e-734a-4560-bd47-d38ad115a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figuring out DBSCAN optimal eps value\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "k = 50 \n",
    "nbrs = NearestNeighbors(n_neighbors=k).fit(embeddings_pca)\n",
    "distances, indices = nbrs.kneighbors(embeddings_pca)\n",
    "\n",
    "# We focus on the distance to the k-th nearest neighbor\n",
    "distance_desc = sorted(distances[:, k-1], reverse=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(distance_desc)\n",
    "plt.title('K-Distance Graph (Finding the Elbow for eps)')\n",
    "plt.ylabel('Distance (eps)')\n",
    "plt.xlabel('Points sorted by distance')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9b0dc7-9866-427c-8877-bd7243a533c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN Configuration \n",
    "EPS_VALUE = 0.8\n",
    "MIN_SAMPLES = 20\n",
    "\n",
    "print(f\"Running DBSCAN (eps={EPS_VALUE}, min_samples={MIN_SAMPLES})...\")\n",
    "dbscan = DBSCAN(eps=EPS_VALUE, min_samples=MIN_SAMPLES)\n",
    "dbscan_labels = dbscan.fit_predict(embeddings_pca)\n",
    "\n",
    "# Add to dataframe\n",
    "df_sample['dbscan_cluster'] = dbscan_labels\n",
    "\n",
    "# Count clusters (excluding noise -1)\n",
    "n_clusters_db = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "noise_percent = (n_noise / len(df_sample)) * 100\n",
    "\n",
    "print(\"DBSCAN Results\")\n",
    "print(f\"Estimated number of clusters: {n_clusters_db}\")\n",
    "print(f\"Noise points (Cluster -1): {n_noise} ({noise_percent:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
